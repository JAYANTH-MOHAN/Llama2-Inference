{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "459bbf0e-a68d-408c-b4cc-ffce9aa3f088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aeed6b0-1bde-4990-87ae-ff7cd2940930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "access_token_read = \"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\"\n",
    "login(token = access_token_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4aaa02-8860-4a40-ab1a-d6e69c888c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db9c9743-1a23-4890-9f78-d19650149742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43dc9920bb84458a9561a6a461ed8473",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model,load_in_8bit=True,device_map=\"auto\")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", #task\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    "    max_length=450,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a707e8eb-e7ea-4f41-bc4b-1ff822d4fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=HuggingFacePipeline(pipeline=pipeline,model_kwargs={'temperature':0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b318c91-43b7-43b3-bd4e-bb140fe58c6e",
   "metadata": {},
   "source": [
    "## Template 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06cabe48-fba7-45c1-b50d-037c04b45a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(Please select the answer according to the above options)\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are a Intent Classifier for Customers based on the answer provided by the bot classify their intent into any one of this category\n",
    "\"Busy\", \"Didn't Hear\", \"Repeat\", \"Confirms\", \"Denies\", \"None of These\".\n",
    "\n",
    "A1: {question}\n",
    "\n",
    "A2: \"{answer}\"\n",
    "\n",
    "Intent Classification: Based on the A1's question, classify the intent of the A2's answer using the following options:\n",
    "\n",
    "1. Busy\n",
    "2. Didn't Hear\n",
    "3. Repeat\n",
    "4. Confirms\n",
    "5. Denies\n",
    "6. None of These\n",
    "\n",
    "Type the corresponding number (1-6) to indicate the intent category of the A2's answer.\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"answer\"])\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Example interaction\n",
    "bot_question = \"Can you confirm your availability for the meeting?\"\n",
    "receiver_answer = \"I am busy at that time, can we reschedule?\"\n",
    "\n",
    "# Use the LLMChain to get the response\n",
    "response = llm_chain.run(question=bot_question, answer=receiver_answer)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0245d507-9584-4ae8-b94e-80f9c0cf0d39",
   "metadata": {},
   "source": [
    "## Template 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41d57ac4-2f24-4c05-b600-abd3668638dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please select an answer from the options provided.\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "You are a Intent Classifier for Customers based on the answer provided by the bot classify their intent into any one of this category\n",
    "\"Busy\", \"Didn't Hear\", \"Repeat\", \"Confirms\", \"Denies\", \"None of These\".\n",
    "Bot: {question}\n",
    "\n",
    "Receiver's Answer: \"{answer}\"\n",
    "\n",
    "Choose one that matches Receiver'sanswer's intent \"Busy\", \"Didn't Hear\", \"Repeat\", \"Confirms\", \"Denies\", \"None of These\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"answer\"])\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Example interaction\n",
    "bot_question = \"Can you confirm your availability for the meeting?\"\n",
    "receiver_answer = \"Thank you\"\n",
    "\n",
    "# Use the LLMChain to get the response\n",
    "response = llm_chain.run(question=bot_question, answer=receiver_answer)\n",
    "print(response)\n",
    "# Force the LLM to select one intent answer from the available options\n",
    "#selected_intent_number = int(response.strip())\n",
    "\n",
    "# Map the selected number to the corresponding intent category\n",
    "#intents = [\"Busy\", \"Didn't Hear\", \"Repeat\", \"Confirms\", \"Denies\", \"None of These\"]\n",
    "#selected_intent = intents[selected_intent_number - 1]\n",
    "\n",
    "#print(\"Selected Intent:\", selected_intent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c454d26-b0cd-474e-8065-099ae7501e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the user's response, the intent of the user is \"Confirms\".\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "\n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "You are supposed to behave as intent classifier. THere is a bot which will ask question to the user and based on user response classify the response as \n",
    "intent is : \"Busy\", \"Didn't Hear\", \"Repeat\", \"Confirms\", \"Denies\", \"None of These\".\n",
    "If you don't know the answer to a question, please don't share false information tell 'None of The Above'.\n",
    "<</SYS>>\n",
    "\n",
    "Bot : {question}\n",
    "user_response : {answer} [/INST]\n",
    "Intent:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"answer\"])\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Example interaction\n",
    "bot_question = \"Can you confirm your availability for the meeting?\"\n",
    "receiver_answer = \"Yeah Sure\"\n",
    "\n",
    "# Use the LLMChain to get the response\n",
    "response = llm_chain.run(question=bot_question, answer=receiver_answer)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10500f4c-90f9-4504-b5eb-b963eb49b4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236c90a-5f38-4b68-92c0-59fe749172a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cfda0e-929a-4c70-82a9-8bfe577c3a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        stop_ids = [29, 0]\n",
    "        for stop_id in stop_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "def predict(message, history): \n",
    "\n",
    "    history_transformer_format = history + [[message, \"\"]]\n",
    "    stop = StopOnTokens()\n",
    "\n",
    "    messages = \"\".join([\"\".join([\"\\n<human>:\"+item[0], \"\\n<bot>:\"+item[1]])  #curr_system_message + \n",
    "                for item in history_transformer_format])\n",
    "    \n",
    "    model_inputs = tokenizer([messages], return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        model_inputs,\n",
    "        streamer=streamer,\n",
    "        max_new_tokens=24,\n",
    "        do_sample=True,\n",
    "        top_p=0.95,\n",
    "        top_k=1000,\n",
    "        temperature=1.0,\n",
    "        num_beams=1,\n",
    "        stopping_criteria=StoppingCriteriaList([stop])\n",
    "        )\n",
    "    t = Thread(target=model.generate, kwargs=generate_kwargs)\n",
    "    t.start()\n",
    "\n",
    "    partial_message  = \"\"\n",
    "    for new_token in streamer:\n",
    "        if new_token != '<':\n",
    "            partial_message += new_token\n",
    "            yield partial_message \n",
    "            \n",
    "\n",
    "gr.ChatInterface(predict).queue().launch(share=True)\n",
    "\n",
    "## Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71d0bf85-b3de-4691-b520-5cc3587074aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "The safetensors archive passed at /home/ubuntu/.cache/huggingface/hub/models--TheBloke--Llama-2-7b-Chat-GPTQ/snapshots/b7ee6c20ac0bba85a310dc699d6bb4c845811608/gptq_model-4bit-128g.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
      "skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Tell me about AI [/INST]  Of course, I'd be happy to help you learn more about AI! Artificial Intelligence (AI) is a rapidly growing field that involves the development of computer systems that can perform tasks that typically require human intelligence, such as visual perception, speech recognition, and decision-making.\n",
      "There are several types of AI, including:\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as playing chess or recognizing faces. Narrow AI is the most common type of AI and is used in many applications, such as virtual assistants, self-driving cars, and medical diagnosis.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can, such as reasoning, problem-solving, and learning. General AI is still in the early stages of development and is not yet widely available.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds. Superintelligence is still largely theoretical and is not yet possible with current technology.\n",
      "4. Artificial general intelligence (AGI): This type of AI is designed to perform any intellectual task that a human can, and is considered the holy grail of AI research. AGI is still in the early stages of development and is not yet widely available.\n",
      "5. Cognitive computing: This type of AI is designed to mimic human thought processes, such as reasoning, problem-solving, and learning. Cognitive computing is used in many applications, such as customer service, financial analysis, and medical diagnosis.\n",
      "6. Machine learning: This type of AI is designed to learn from data and improve its performance over time. Machine learning is used in many applications, such as image and speech recognition, natural language processing, and recommendation systems.\n",
      "7. Natural language processing (NLP): This type of AI is designed to understand, interpret, and generate human language. NLP is used in many applications, such as chatbots, voice assistants, and language translation.\n",
      "8. Robotics: This type of AI is designed to interact with the physical world, such as robots, drones, and autonomous vehicles. Robotics is used in many applications, such as manufacturing, logistics, and healthcare.\n",
      "9. Computer vision:\n",
      "*** Pipeline:\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "\n",
      "Tell me about AI [/INST]  Of course! I'd be happy to provide information on AI (Artificial Intelligence). AI refers to the development of computer systems able to perform tasks typically requiring human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Artificial intelligence has been around for several decades and has evolved significantly over time. Here are some key aspects of AI:\n",
      "1. Machine Learning: A subset of AI focused on enabling machines to learn from data without explicit programming. It involves training algorithms using historical data to recognize patterns, classify objects, or predict outcomes.\n",
      "2. Deep Learning: A subfield of machine learning that utilizes neural networks with multiple layers to analyze complex data sets. These networks can recognize images, understand natural language, and generate creative content like music or art.\n",
      "3. Natural Language Processing (NLP): The branch of AI concerned with developing computers capable of understanding, interpreting, and generating human language. NLP enables applications like chatbots, voice assistants, and language translation software.\n",
      "4. Robotics: The intersection of AI and robotics focuses on creating robots capable of performing tasks that typically require human intelligence, such as assembly, maintenance, and transportation.\n",
      "5. Computer Vision: This area of AI deals with enabling computers to interpret and understand visual data from the world, including recognizing objects, tracking movements, and analyzing facial expressions.\n",
      "6. Expert Systems: These are AI systems designed to mimic the decision-making abilities of human experts in specific domains, such as medicine, finance, or engineering.\n",
      "7. Reinforcement Learning: An AI technique where an algorithm learns by interacting with its environment and receiving feedback in the form of rewards or penalties. This approach allows AI to optimize its behavior based on desired outcomes.\n",
      "8. Generative Adversarial Networks (GANs): A type of deep learning algorithm involving two neural networks working together to create new data that resembles existing examples. GANs have led to breakthroughs in image generation, video creation, and text production.\n",
      "9. Autonomous Vehicles: Self-driving cars and trucks use a combination of sensors, mapping technology, and AI algorithms to navigate roads safely and efficiently.\n",
      "10. Ethical Considerations: As A\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline, logging\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "\n",
    "model_name_or_path = \"TheBloke/Llama-2-7b-Chat-GPTQ\"\n",
    "model_basename = \"gptq_model-4bit-128g\"\n",
    "\n",
    "use_triton = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        use_triton=use_triton,\n",
    "        quantize_config=None)\n",
    "\n",
    "\"\"\"\n",
    "To download from a specific branch, use the revision parameter, as in this example:\n",
    "\n",
    "model = AutoGPTQForCausalLM.from_quantized(model_name_or_path,\n",
    "        revision=\"gptq-4bit-32g-actorder_True\",\n",
    "        model_basename=model_basename,\n",
    "        use_safetensors=True,\n",
    "        trust_remote_code=True,\n",
    "        device=\"cuda:0\",\n",
    "        quantize_config=None)\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"Tell me about AI\"\n",
    "system_message = \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "{system_message}\n",
    "<</SYS>>\n",
    "\n",
    "{prompt} [/INST]'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "# Prevent printing spurious transformers error when using pipeline with AutoGPTQ\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.15\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df381a9-11a4-4a1e-ba10-db232c6b8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://8db693ee5ae892f3eb.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8db693ee5ae892f3eb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import PromptTemplate, LLMChain\n",
    "import gradio as gr\n",
    "\n",
    "template = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
    "\n",
    "You are supposed to behave as an intent classifier. There is a bot that will ask questions to the user, and based on the user response, you should classify the response as \"Busy\", \"Didn't Hear\", \"Repeat\", \"Confirms\", \"Denies\", or \"None of These\".\n",
    "If you don't know the answer to a question, please don't share false information. Tell 'None of The Above'.\n",
    "<</SYS>>\n",
    "\n",
    "Bot : {question}\n",
    "user_response : {answer} [/INST]\n",
    "Intent:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\", \"answer\"])\n",
    "\n",
    "# Create the LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Wrap LLMChain logic in a function for intent classification\n",
    "def classify_intent(bot_question, receiver_answer):\n",
    "    response = llm_chain.run(question=bot_question, answer=receiver_answer)\n",
    "    return response\n",
    "\n",
    "# Create a Gradio interface\n",
    "gr_interface = gr.Interface(\n",
    "    fn=classify_intent,\n",
    "    inputs=[\"text\", \"text\"],  # Two text inputs: bot_question and receiver_answer\n",
    "    outputs=\"text\"  # Output will be the intent prediction\n",
    ")\n",
    "\n",
    "# Launch the Gradio interface\n",
    "gr_interface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83969d9f-6338-45f2-a6f3-57dc7b896719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "llama2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
